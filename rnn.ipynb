{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e63e70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import os\n",
    "import time\n",
    "import random as rn\n",
    "import scipy as sp\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from torchvision import transforms\n",
    "import snntorch as snn\n",
    "from snntorch import utils\n",
    "from torch.utils.data import DataLoader\n",
    "from snntorch import spikegen\n",
    "\n",
    "\n",
    "# Read the CSV file\n",
    "data_frame = pd.read_csv('/home/saqib/projects/neuromorphicComputing/emg_data_class1.csv')\n",
    "\n",
    "# Convert the DataFrame to a tensor\n",
    "class1_data = torch.tensor(data_frame.values, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# Read the CSV file\n",
    "data_frame = pd.read_csv('/home/saqib/projects/neuromorphicComputing/emg_data_class2.csv')\n",
    "\n",
    "# Convert the DataFrame to a tensor\n",
    "class2_data = torch.tensor(data_frame.values, dtype=torch.float32)\n",
    "\n",
    "# Assign labels to each class\n",
    "class1_labels = torch.zeros(500, dtype=torch.long)\n",
    "class2_labels = torch.ones(500, dtype=torch.long)\n",
    "\n",
    "# Parameters\n",
    "num_samples = 1000  # Number of time steps\n",
    "num_channels = 8  # Number of EMG channels\n",
    "hidden_size = 64  # Number of hidden units in the RNN layer\n",
    "num_classes = 2  # Number of output classes\n",
    "\n",
    "# Concatenate the data and labels\n",
    "data = torch.cat((class1_data, class2_data), dim=0)\n",
    "labels = torch.cat((class1_labels, class2_labels), dim=0)\n",
    "\n",
    "# Reshape the data for RNN compatibility\n",
    "data = data.unsqueeze(dim=0)  # Add a batch dimension\n",
    "data = data.permute(1, 0, 2)  # Reshape to (sequence_length, batch_size, num_channels)\n",
    "\n",
    "# Ensure labels have the same batch size as data\n",
    "labels = labels.unsqueeze(dim=0)\n",
    "\n",
    "# Define the RNN model\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, num_channels, hidden_size, num_classes):\n",
    "        super(RNN, self).__init__()\n",
    "        self.rnn = nn.RNN(num_channels, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Instantiate the RNN model\n",
    "rnn = RNN(num_channels, hidden_size, num_classes)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(rnn.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "epochs_list=[10,50,100,300,500,700,1000,1500,2000,3000,4000,5000,6000,7000,8000,9000,10000]\n",
    "losses_list=[]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "456de964",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10000], Loss: 0.6415\n",
      "Epoch [20/10000], Loss: 0.6416\n",
      "Epoch [30/10000], Loss: 0.6414\n",
      "Epoch [40/10000], Loss: 0.6414\n",
      "Epoch [50/10000], Loss: 0.6414\n",
      "Epoch [60/10000], Loss: 0.6413\n",
      "Epoch [70/10000], Loss: 0.6413\n",
      "Epoch [80/10000], Loss: 0.6413\n",
      "Epoch [90/10000], Loss: 0.6413\n",
      "Epoch [100/10000], Loss: 0.6413\n",
      "Epoch [110/10000], Loss: 0.6413\n",
      "Epoch [120/10000], Loss: 0.6413\n",
      "Epoch [130/10000], Loss: 0.6413\n",
      "Epoch [140/10000], Loss: 0.6413\n",
      "Epoch [150/10000], Loss: 0.6413\n",
      "Epoch [160/10000], Loss: 0.6413\n",
      "Epoch [170/10000], Loss: 0.6413\n",
      "Epoch [180/10000], Loss: 0.6413\n",
      "Epoch [190/10000], Loss: 0.6413\n",
      "Epoch [200/10000], Loss: 0.6412\n",
      "Epoch [210/10000], Loss: 0.6412\n",
      "Epoch [220/10000], Loss: 0.6412\n",
      "Epoch [230/10000], Loss: 0.6412\n",
      "Epoch [240/10000], Loss: 0.6412\n",
      "Epoch [250/10000], Loss: 0.6412\n",
      "Epoch [260/10000], Loss: 0.6412\n",
      "Epoch [270/10000], Loss: 0.6412\n",
      "Epoch [280/10000], Loss: 0.6425\n",
      "Epoch [290/10000], Loss: 0.6412\n",
      "Epoch [300/10000], Loss: 0.6412\n",
      "Epoch [310/10000], Loss: 0.6412\n",
      "Epoch [320/10000], Loss: 0.6412\n",
      "Epoch [330/10000], Loss: 0.6412\n",
      "Epoch [340/10000], Loss: 0.6412\n",
      "Epoch [350/10000], Loss: 0.6411\n",
      "Epoch [360/10000], Loss: 0.6411\n",
      "Epoch [370/10000], Loss: 0.6411\n",
      "Epoch [380/10000], Loss: 0.6411\n",
      "Epoch [390/10000], Loss: 0.6411\n",
      "Epoch [400/10000], Loss: 0.6411\n",
      "Epoch [410/10000], Loss: 0.6411\n",
      "Epoch [420/10000], Loss: 0.6411\n",
      "Epoch [430/10000], Loss: 0.6411\n",
      "Epoch [440/10000], Loss: 0.6411\n",
      "Epoch [450/10000], Loss: 0.6411\n",
      "Epoch [460/10000], Loss: 0.6411\n",
      "Epoch [470/10000], Loss: 0.6411\n",
      "Epoch [480/10000], Loss: 0.6411\n",
      "Epoch [490/10000], Loss: 0.6410\n",
      "Epoch [500/10000], Loss: 0.6410\n",
      "Epoch [510/10000], Loss: 0.6410\n",
      "Epoch [520/10000], Loss: 0.6410\n",
      "Epoch [530/10000], Loss: 0.6410\n",
      "Epoch [540/10000], Loss: 0.6410\n",
      "Epoch [550/10000], Loss: 0.6410\n",
      "Epoch [560/10000], Loss: 0.6410\n",
      "Epoch [570/10000], Loss: 0.6410\n",
      "Epoch [580/10000], Loss: 0.6413\n",
      "Epoch [590/10000], Loss: 0.6413\n",
      "Epoch [600/10000], Loss: 0.6410\n",
      "Epoch [610/10000], Loss: 0.6410\n",
      "Epoch [620/10000], Loss: 0.6410\n",
      "Epoch [630/10000], Loss: 0.6409\n",
      "Epoch [640/10000], Loss: 0.6409\n",
      "Epoch [650/10000], Loss: 0.6409\n",
      "Epoch [660/10000], Loss: 0.6409\n",
      "Epoch [670/10000], Loss: 0.6409\n",
      "Epoch [680/10000], Loss: 0.6409\n",
      "Epoch [690/10000], Loss: 0.6409\n",
      "Epoch [700/10000], Loss: 0.6409\n",
      "Epoch [710/10000], Loss: 0.6409\n",
      "Epoch [720/10000], Loss: 0.6409\n",
      "Epoch [730/10000], Loss: 0.6409\n",
      "Epoch [740/10000], Loss: 0.6409\n",
      "Epoch [750/10000], Loss: 0.6409\n",
      "Epoch [760/10000], Loss: 0.6409\n",
      "Epoch [770/10000], Loss: 0.6408\n",
      "Epoch [780/10000], Loss: 0.6408\n",
      "Epoch [790/10000], Loss: 0.6408\n",
      "Epoch [800/10000], Loss: 0.6408\n",
      "Epoch [810/10000], Loss: 0.6409\n",
      "Epoch [820/10000], Loss: 0.6409\n",
      "Epoch [830/10000], Loss: 0.6410\n",
      "Epoch [840/10000], Loss: 0.6408\n",
      "Epoch [850/10000], Loss: 0.6408\n",
      "Epoch [860/10000], Loss: 0.6408\n",
      "Epoch [870/10000], Loss: 0.6408\n",
      "Epoch [880/10000], Loss: 0.6408\n",
      "Epoch [890/10000], Loss: 0.6408\n",
      "Epoch [900/10000], Loss: 0.6408\n",
      "Epoch [910/10000], Loss: 0.6407\n",
      "Epoch [920/10000], Loss: 0.6407\n",
      "Epoch [930/10000], Loss: 0.6407\n",
      "Epoch [940/10000], Loss: 0.6407\n",
      "Epoch [950/10000], Loss: 0.6407\n",
      "Epoch [960/10000], Loss: 0.6407\n",
      "Epoch [970/10000], Loss: 0.6407\n",
      "Epoch [980/10000], Loss: 0.6407\n",
      "Epoch [990/10000], Loss: 0.6407\n",
      "Epoch [1000/10000], Loss: 0.6407\n",
      "Epoch [1010/10000], Loss: 0.6407\n",
      "Epoch [1020/10000], Loss: 0.6407\n",
      "Epoch [1030/10000], Loss: 0.6407\n",
      "Epoch [1040/10000], Loss: 0.6406\n",
      "Epoch [1050/10000], Loss: 0.6406\n",
      "Epoch [1060/10000], Loss: 0.6416\n",
      "Epoch [1070/10000], Loss: 0.6408\n",
      "Epoch [1080/10000], Loss: 0.6407\n",
      "Epoch [1090/10000], Loss: 0.6407\n",
      "Epoch [1100/10000], Loss: 0.6406\n",
      "Epoch [1110/10000], Loss: 0.6406\n",
      "Epoch [1120/10000], Loss: 0.6406\n",
      "Epoch [1130/10000], Loss: 0.6406\n",
      "Epoch [1140/10000], Loss: 0.6406\n",
      "Epoch [1150/10000], Loss: 0.6406\n",
      "Epoch [1160/10000], Loss: 0.6406\n",
      "Epoch [1170/10000], Loss: 0.6406\n",
      "Epoch [1180/10000], Loss: 0.6405\n",
      "Epoch [1190/10000], Loss: 0.6405\n",
      "Epoch [1200/10000], Loss: 0.6405\n",
      "Epoch [1210/10000], Loss: 0.6405\n",
      "Epoch [1220/10000], Loss: 0.6405\n",
      "Epoch [1230/10000], Loss: 0.6405\n",
      "Epoch [1240/10000], Loss: 0.6405\n",
      "Epoch [1250/10000], Loss: 0.6405\n",
      "Epoch [1260/10000], Loss: 0.6405\n",
      "Epoch [1270/10000], Loss: 0.6405\n",
      "Epoch [1280/10000], Loss: 0.6405\n",
      "Epoch [1290/10000], Loss: 0.6405\n",
      "Epoch [1300/10000], Loss: 0.6405\n",
      "Epoch [1310/10000], Loss: 0.6404\n",
      "Epoch [1320/10000], Loss: 0.6404\n",
      "Epoch [1330/10000], Loss: 0.6404\n",
      "Epoch [1340/10000], Loss: 0.6405\n",
      "Epoch [1350/10000], Loss: 0.6412\n",
      "Epoch [1360/10000], Loss: 0.6407\n",
      "Epoch [1370/10000], Loss: 0.6405\n",
      "Epoch [1380/10000], Loss: 0.6404\n",
      "Epoch [1390/10000], Loss: 0.6404\n",
      "Epoch [1400/10000], Loss: 0.6404\n",
      "Epoch [1410/10000], Loss: 0.6404\n",
      "Epoch [1420/10000], Loss: 0.6404\n",
      "Epoch [1430/10000], Loss: 0.6404\n",
      "Epoch [1440/10000], Loss: 0.6404\n",
      "Epoch [1450/10000], Loss: 0.6403\n",
      "Epoch [1460/10000], Loss: 0.6403\n",
      "Epoch [1470/10000], Loss: 0.6403\n",
      "Epoch [1480/10000], Loss: 0.6403\n",
      "Epoch [1490/10000], Loss: 0.6403\n",
      "Epoch [1500/10000], Loss: 0.6403\n",
      "Epoch [1510/10000], Loss: 0.6403\n",
      "Epoch [1520/10000], Loss: 0.6403\n",
      "Epoch [1530/10000], Loss: 0.6403\n",
      "Epoch [1540/10000], Loss: 0.6403\n",
      "Epoch [1550/10000], Loss: 0.6403\n",
      "Epoch [1560/10000], Loss: 0.6403\n",
      "Epoch [1570/10000], Loss: 0.6403\n",
      "Epoch [1580/10000], Loss: 0.6402\n",
      "Epoch [1590/10000], Loss: 0.6402\n",
      "Epoch [1600/10000], Loss: 0.6402\n",
      "Epoch [1610/10000], Loss: 0.6402\n",
      "Epoch [1620/10000], Loss: 0.6402\n",
      "Epoch [1630/10000], Loss: 0.6402\n",
      "Epoch [1640/10000], Loss: 0.6402\n",
      "Epoch [1650/10000], Loss: 0.6402\n",
      "Epoch [1660/10000], Loss: 0.6402\n",
      "Epoch [1670/10000], Loss: 0.6402\n",
      "Epoch [1680/10000], Loss: 0.6410\n",
      "Epoch [1690/10000], Loss: 0.6405\n",
      "Epoch [1700/10000], Loss: 0.6403\n",
      "Epoch [1710/10000], Loss: 0.6402\n",
      "Epoch [1720/10000], Loss: 0.6402\n",
      "Epoch [1730/10000], Loss: 0.6401\n",
      "Epoch [1740/10000], Loss: 0.6401\n",
      "Epoch [1750/10000], Loss: 0.6401\n",
      "Epoch [1760/10000], Loss: 0.6401\n",
      "Epoch [1770/10000], Loss: 0.6401\n",
      "Epoch [1780/10000], Loss: 0.6401\n",
      "Epoch [1790/10000], Loss: 0.6401\n",
      "Epoch [1800/10000], Loss: 0.6401\n",
      "Epoch [1810/10000], Loss: 0.6401\n",
      "Epoch [1820/10000], Loss: 0.6401\n",
      "Epoch [1830/10000], Loss: 0.6401\n",
      "Epoch [1840/10000], Loss: 0.6401\n",
      "Epoch [1850/10000], Loss: 0.6400\n",
      "Epoch [1860/10000], Loss: 0.6400\n",
      "Epoch [1870/10000], Loss: 0.6400\n",
      "Epoch [1880/10000], Loss: 0.6400\n",
      "Epoch [1890/10000], Loss: 0.6400\n",
      "Epoch [1900/10000], Loss: 0.6400\n",
      "Epoch [1910/10000], Loss: 0.6400\n",
      "Epoch [1920/10000], Loss: 0.6400\n",
      "Epoch [1930/10000], Loss: 0.6400\n",
      "Epoch [1940/10000], Loss: 0.6400\n",
      "Epoch [1950/10000], Loss: 0.6400\n",
      "Epoch [1960/10000], Loss: 0.6401\n",
      "Epoch [1970/10000], Loss: 0.6400\n",
      "Epoch [1980/10000], Loss: 0.6400\n",
      "Epoch [1990/10000], Loss: 0.6400\n",
      "Epoch [2000/10000], Loss: 0.6400\n",
      "Epoch [2010/10000], Loss: 0.6399\n",
      "Epoch [2020/10000], Loss: 0.6399\n",
      "Epoch [2030/10000], Loss: 0.6399\n",
      "Epoch [2040/10000], Loss: 0.6399\n",
      "Epoch [2050/10000], Loss: 0.6399\n",
      "Epoch [2060/10000], Loss: 0.6399\n",
      "Epoch [2070/10000], Loss: 0.6399\n",
      "Epoch [2080/10000], Loss: 0.6399\n",
      "Epoch [2090/10000], Loss: 0.6399\n",
      "Epoch [2100/10000], Loss: 0.6399\n",
      "Epoch [2110/10000], Loss: 0.6398\n",
      "Epoch [2120/10000], Loss: 0.6398\n",
      "Epoch [2130/10000], Loss: 0.6398\n",
      "Epoch [2140/10000], Loss: 0.6398\n",
      "Epoch [2150/10000], Loss: 0.6398\n",
      "Epoch [2160/10000], Loss: 0.6398\n",
      "Epoch [2170/10000], Loss: 0.6398\n",
      "Epoch [2180/10000], Loss: 0.6398\n",
      "Epoch [2190/10000], Loss: 0.6398\n",
      "Epoch [2200/10000], Loss: 0.6398\n",
      "Epoch [2210/10000], Loss: 0.6399\n",
      "Epoch [2220/10000], Loss: 0.6398\n",
      "Epoch [2230/10000], Loss: 0.6399\n",
      "Epoch [2240/10000], Loss: 0.6398\n",
      "Epoch [2250/10000], Loss: 0.6397\n",
      "Epoch [2260/10000], Loss: 0.6397\n",
      "Epoch [2270/10000], Loss: 0.6397\n",
      "Epoch [2280/10000], Loss: 0.6397\n",
      "Epoch [2290/10000], Loss: 0.6397\n",
      "Epoch [2300/10000], Loss: 0.6397\n",
      "Epoch [2310/10000], Loss: 0.6397\n",
      "Epoch [2320/10000], Loss: 0.6397\n",
      "Epoch [2330/10000], Loss: 0.6397\n",
      "Epoch [2340/10000], Loss: 0.6397\n",
      "Epoch [2350/10000], Loss: 0.6397\n",
      "Epoch [2360/10000], Loss: 0.6396\n",
      "Epoch [2370/10000], Loss: 0.6396\n",
      "Epoch [2380/10000], Loss: 0.6396\n",
      "Epoch [2390/10000], Loss: 0.6396\n",
      "Epoch [2400/10000], Loss: 0.6396\n",
      "Epoch [2410/10000], Loss: 0.6396\n",
      "Epoch [2420/10000], Loss: 0.6396\n",
      "Epoch [2430/10000], Loss: 0.6396\n",
      "Epoch [2440/10000], Loss: 0.6396\n",
      "Epoch [2450/10000], Loss: 0.6400\n",
      "Epoch [2460/10000], Loss: 0.6401\n",
      "Epoch [2470/10000], Loss: 0.6397\n",
      "Epoch [2480/10000], Loss: 0.6396\n",
      "Epoch [2490/10000], Loss: 0.6395\n",
      "Epoch [2500/10000], Loss: 0.6395\n",
      "Epoch [2510/10000], Loss: 0.6395\n",
      "Epoch [2520/10000], Loss: 0.6395\n",
      "Epoch [2530/10000], Loss: 0.6395\n",
      "Epoch [2540/10000], Loss: 0.6395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2550/10000], Loss: 0.6395\n",
      "Epoch [2560/10000], Loss: 0.6395\n",
      "Epoch [2570/10000], Loss: 0.6395\n",
      "Epoch [2580/10000], Loss: 0.6395\n",
      "Epoch [2590/10000], Loss: 0.6395\n",
      "Epoch [2600/10000], Loss: 0.6395\n",
      "Epoch [2610/10000], Loss: 0.6394\n",
      "Epoch [2620/10000], Loss: 0.6394\n",
      "Epoch [2630/10000], Loss: 0.6394\n",
      "Epoch [2640/10000], Loss: 0.6394\n",
      "Epoch [2650/10000], Loss: 0.6394\n",
      "Epoch [2660/10000], Loss: 0.6394\n",
      "Epoch [2670/10000], Loss: 0.6394\n",
      "Epoch [2680/10000], Loss: 0.6394\n",
      "Epoch [2690/10000], Loss: 0.6394\n",
      "Epoch [2700/10000], Loss: 0.6394\n",
      "Epoch [2710/10000], Loss: 0.6394\n",
      "Epoch [2720/10000], Loss: 0.6394\n",
      "Epoch [2730/10000], Loss: 0.6401\n",
      "Epoch [2740/10000], Loss: 0.6395\n",
      "Epoch [2750/10000], Loss: 0.6393\n",
      "Epoch [2760/10000], Loss: 0.6393\n",
      "Epoch [2770/10000], Loss: 0.6393\n",
      "Epoch [2780/10000], Loss: 0.6393\n",
      "Epoch [2790/10000], Loss: 0.6393\n",
      "Epoch [2800/10000], Loss: 0.6393\n",
      "Epoch [2810/10000], Loss: 0.6393\n",
      "Epoch [2820/10000], Loss: 0.6393\n",
      "Epoch [2830/10000], Loss: 0.6393\n",
      "Epoch [2840/10000], Loss: 0.6393\n",
      "Epoch [2850/10000], Loss: 0.6393\n",
      "Epoch [2860/10000], Loss: 0.6392\n",
      "Epoch [2870/10000], Loss: 0.6392\n",
      "Epoch [2880/10000], Loss: 0.6392\n",
      "Epoch [2890/10000], Loss: 0.6392\n",
      "Epoch [2900/10000], Loss: 0.6392\n",
      "Epoch [2910/10000], Loss: 0.6392\n",
      "Epoch [2920/10000], Loss: 0.6392\n",
      "Epoch [2930/10000], Loss: 0.6392\n",
      "Epoch [2940/10000], Loss: 0.6392\n",
      "Epoch [2950/10000], Loss: 0.6392\n",
      "Epoch [2960/10000], Loss: 0.6392\n",
      "Epoch [2970/10000], Loss: 0.6392\n",
      "Epoch [2980/10000], Loss: 0.6394\n",
      "Epoch [2990/10000], Loss: 0.6394\n",
      "Epoch [3000/10000], Loss: 0.6391\n",
      "Epoch [3010/10000], Loss: 0.6392\n",
      "Epoch [3020/10000], Loss: 0.6391\n",
      "Epoch [3030/10000], Loss: 0.6391\n",
      "Epoch [3040/10000], Loss: 0.6391\n",
      "Epoch [3050/10000], Loss: 0.6391\n",
      "Epoch [3060/10000], Loss: 0.6391\n",
      "Epoch [3070/10000], Loss: 0.6391\n",
      "Epoch [3080/10000], Loss: 0.6391\n",
      "Epoch [3090/10000], Loss: 0.6391\n",
      "Epoch [3100/10000], Loss: 0.6391\n",
      "Epoch [3110/10000], Loss: 0.6390\n",
      "Epoch [3120/10000], Loss: 0.6390\n",
      "Epoch [3130/10000], Loss: 0.6390\n",
      "Epoch [3140/10000], Loss: 0.6390\n",
      "Epoch [3150/10000], Loss: 0.6390\n",
      "Epoch [3160/10000], Loss: 0.6390\n",
      "Epoch [3170/10000], Loss: 0.6390\n",
      "Epoch [3180/10000], Loss: 0.6390\n",
      "Epoch [3190/10000], Loss: 0.6390\n",
      "Epoch [3200/10000], Loss: 0.6390\n",
      "Epoch [3210/10000], Loss: 0.6390\n",
      "Epoch [3220/10000], Loss: 0.6390\n",
      "Epoch [3230/10000], Loss: 0.6396\n",
      "Epoch [3240/10000], Loss: 0.6393\n",
      "Epoch [3250/10000], Loss: 0.6390\n",
      "Epoch [3260/10000], Loss: 0.6389\n",
      "Epoch [3270/10000], Loss: 0.6389\n",
      "Epoch [3280/10000], Loss: 0.6389\n",
      "Epoch [3290/10000], Loss: 0.6389\n",
      "Epoch [3300/10000], Loss: 0.6389\n",
      "Epoch [3310/10000], Loss: 0.6389\n",
      "Epoch [3320/10000], Loss: 0.6389\n",
      "Epoch [3330/10000], Loss: 0.6389\n",
      "Epoch [3340/10000], Loss: 0.6389\n",
      "Epoch [3350/10000], Loss: 0.6388\n",
      "Epoch [3360/10000], Loss: 0.6388\n",
      "Epoch [3370/10000], Loss: 0.6388\n",
      "Epoch [3380/10000], Loss: 0.6388\n",
      "Epoch [3390/10000], Loss: 0.6388\n",
      "Epoch [3400/10000], Loss: 0.6388\n",
      "Epoch [3410/10000], Loss: 0.6388\n",
      "Epoch [3420/10000], Loss: 0.6388\n",
      "Epoch [3430/10000], Loss: 0.6388\n",
      "Epoch [3440/10000], Loss: 0.6388\n",
      "Epoch [3450/10000], Loss: 0.6388\n",
      "Epoch [3460/10000], Loss: 0.6388\n",
      "Epoch [3470/10000], Loss: 0.6389\n",
      "Epoch [3480/10000], Loss: 0.6388\n",
      "Epoch [3490/10000], Loss: 0.6388\n",
      "Epoch [3500/10000], Loss: 0.6388\n",
      "Epoch [3510/10000], Loss: 0.6387\n",
      "Epoch [3520/10000], Loss: 0.6387\n",
      "Epoch [3530/10000], Loss: 0.6387\n",
      "Epoch [3540/10000], Loss: 0.6387\n",
      "Epoch [3550/10000], Loss: 0.6387\n",
      "Epoch [3560/10000], Loss: 0.6387\n",
      "Epoch [3570/10000], Loss: 0.6387\n",
      "Epoch [3580/10000], Loss: 0.6387\n",
      "Epoch [3590/10000], Loss: 0.6386\n",
      "Epoch [3600/10000], Loss: 0.6386\n",
      "Epoch [3610/10000], Loss: 0.6386\n",
      "Epoch [3620/10000], Loss: 0.6386\n",
      "Epoch [3630/10000], Loss: 0.6386\n",
      "Epoch [3640/10000], Loss: 0.6386\n",
      "Epoch [3650/10000], Loss: 0.6386\n",
      "Epoch [3660/10000], Loss: 0.6386\n",
      "Epoch [3670/10000], Loss: 0.6386\n",
      "Epoch [3680/10000], Loss: 0.6386\n",
      "Epoch [3690/10000], Loss: 0.6386\n",
      "Epoch [3700/10000], Loss: 0.6386\n",
      "Epoch [3710/10000], Loss: 0.6385\n",
      "Epoch [3720/10000], Loss: 0.6385\n",
      "Epoch [3730/10000], Loss: 0.6385\n",
      "Epoch [3740/10000], Loss: 0.6391\n",
      "Epoch [3750/10000], Loss: 0.6389\n",
      "Epoch [3760/10000], Loss: 0.6385\n",
      "Epoch [3770/10000], Loss: 0.6385\n",
      "Epoch [3780/10000], Loss: 0.6385\n",
      "Epoch [3790/10000], Loss: 0.6385\n",
      "Epoch [3800/10000], Loss: 0.6385\n",
      "Epoch [3810/10000], Loss: 0.6385\n",
      "Epoch [3820/10000], Loss: 0.6384\n",
      "Epoch [3830/10000], Loss: 0.6384\n",
      "Epoch [3840/10000], Loss: 0.6384\n",
      "Epoch [3850/10000], Loss: 0.6384\n",
      "Epoch [3860/10000], Loss: 0.6384\n",
      "Epoch [3870/10000], Loss: 0.6384\n",
      "Epoch [3880/10000], Loss: 0.6384\n",
      "Epoch [3890/10000], Loss: 0.6384\n",
      "Epoch [3900/10000], Loss: 0.6384\n",
      "Epoch [3910/10000], Loss: 0.6384\n",
      "Epoch [3920/10000], Loss: 0.6384\n",
      "Epoch [3930/10000], Loss: 0.6384\n",
      "Epoch [3940/10000], Loss: 0.6383\n",
      "Epoch [3950/10000], Loss: 0.6383\n",
      "Epoch [3960/10000], Loss: 0.6383\n",
      "Epoch [3970/10000], Loss: 0.6383\n",
      "Epoch [3980/10000], Loss: 0.6393\n",
      "Epoch [3990/10000], Loss: 0.6383\n",
      "Epoch [4000/10000], Loss: 0.6384\n",
      "Epoch [4010/10000], Loss: 0.6383\n",
      "Epoch [4020/10000], Loss: 0.6383\n",
      "Epoch [4030/10000], Loss: 0.6383\n",
      "Epoch [4040/10000], Loss: 0.6383\n",
      "Epoch [4050/10000], Loss: 0.6383\n",
      "Epoch [4060/10000], Loss: 0.6382\n",
      "Epoch [4070/10000], Loss: 0.6382\n",
      "Epoch [4080/10000], Loss: 0.6382\n",
      "Epoch [4090/10000], Loss: 0.6382\n",
      "Epoch [4100/10000], Loss: 0.6382\n",
      "Epoch [4110/10000], Loss: 0.6382\n",
      "Epoch [4120/10000], Loss: 0.6382\n",
      "Epoch [4130/10000], Loss: 0.6382\n",
      "Epoch [4140/10000], Loss: 0.6382\n",
      "Epoch [4150/10000], Loss: 0.6382\n",
      "Epoch [4160/10000], Loss: 0.6382\n",
      "Epoch [4170/10000], Loss: 0.6381\n",
      "Epoch [4180/10000], Loss: 0.6381\n",
      "Epoch [4190/10000], Loss: 0.6381\n",
      "Epoch [4200/10000], Loss: 0.6381\n",
      "Epoch [4210/10000], Loss: 0.6381\n",
      "Epoch [4220/10000], Loss: 0.6381\n",
      "Epoch [4230/10000], Loss: 0.6381\n",
      "Epoch [4240/10000], Loss: 0.6388\n",
      "Epoch [4250/10000], Loss: 0.6384\n",
      "Epoch [4260/10000], Loss: 0.6382\n",
      "Epoch [4270/10000], Loss: 0.6381\n",
      "Epoch [4280/10000], Loss: 0.6381\n",
      "Epoch [4290/10000], Loss: 0.6380\n",
      "Epoch [4300/10000], Loss: 0.6380\n",
      "Epoch [4310/10000], Loss: 0.6380\n",
      "Epoch [4320/10000], Loss: 0.6380\n",
      "Epoch [4330/10000], Loss: 0.6380\n",
      "Epoch [4340/10000], Loss: 0.6380\n",
      "Epoch [4350/10000], Loss: 0.6380\n",
      "Epoch [4360/10000], Loss: 0.6380\n",
      "Epoch [4370/10000], Loss: 0.6380\n",
      "Epoch [4380/10000], Loss: 0.6380\n",
      "Epoch [4390/10000], Loss: 0.6380\n",
      "Epoch [4400/10000], Loss: 0.6379\n",
      "Epoch [4410/10000], Loss: 0.6379\n",
      "Epoch [4420/10000], Loss: 0.6379\n",
      "Epoch [4430/10000], Loss: 0.6379\n",
      "Epoch [4440/10000], Loss: 0.6379\n",
      "Epoch [4450/10000], Loss: 0.6379\n",
      "Epoch [4460/10000], Loss: 0.6379\n",
      "Epoch [4470/10000], Loss: 0.6379\n",
      "Epoch [4480/10000], Loss: 0.6379\n",
      "Epoch [4490/10000], Loss: 0.6379\n",
      "Epoch [4500/10000], Loss: 0.6379\n",
      "Epoch [4510/10000], Loss: 0.6378\n",
      "Epoch [4520/10000], Loss: 0.6378\n",
      "Epoch [4530/10000], Loss: 0.6383\n",
      "Epoch [4540/10000], Loss: 0.6383\n",
      "Epoch [4550/10000], Loss: 0.6379\n",
      "Epoch [4560/10000], Loss: 0.6378\n",
      "Epoch [4570/10000], Loss: 0.6378\n",
      "Epoch [4580/10000], Loss: 0.6378\n",
      "Epoch [4590/10000], Loss: 0.6378\n",
      "Epoch [4600/10000], Loss: 0.6378\n",
      "Epoch [4610/10000], Loss: 0.6378\n",
      "Epoch [4620/10000], Loss: 0.6378\n",
      "Epoch [4630/10000], Loss: 0.6377\n",
      "Epoch [4640/10000], Loss: 0.6377\n",
      "Epoch [4650/10000], Loss: 0.6377\n",
      "Epoch [4660/10000], Loss: 0.6377\n",
      "Epoch [4670/10000], Loss: 0.6377\n",
      "Epoch [4680/10000], Loss: 0.6377\n",
      "Epoch [4690/10000], Loss: 0.6377\n",
      "Epoch [4700/10000], Loss: 0.6377\n",
      "Epoch [4710/10000], Loss: 0.6377\n",
      "Epoch [4720/10000], Loss: 0.6377\n",
      "Epoch [4730/10000], Loss: 0.6377\n",
      "Epoch [4740/10000], Loss: 0.6376\n",
      "Epoch [4750/10000], Loss: 0.6376\n",
      "Epoch [4760/10000], Loss: 0.6376\n",
      "Epoch [4770/10000], Loss: 0.6376\n",
      "Epoch [4780/10000], Loss: 0.6377\n",
      "Epoch [4790/10000], Loss: 0.6380\n",
      "Epoch [4800/10000], Loss: 0.6378\n",
      "Epoch [4810/10000], Loss: 0.6376\n",
      "Epoch [4820/10000], Loss: 0.6376\n",
      "Epoch [4830/10000], Loss: 0.6376\n",
      "Epoch [4840/10000], Loss: 0.6376\n",
      "Epoch [4850/10000], Loss: 0.6375\n",
      "Epoch [4860/10000], Loss: 0.6375\n",
      "Epoch [4870/10000], Loss: 0.6375\n",
      "Epoch [4880/10000], Loss: 0.6375\n",
      "Epoch [4890/10000], Loss: 0.6375\n",
      "Epoch [4900/10000], Loss: 0.6375\n",
      "Epoch [4910/10000], Loss: 0.6375\n",
      "Epoch [4920/10000], Loss: 0.6375\n",
      "Epoch [4930/10000], Loss: 0.6375\n",
      "Epoch [4940/10000], Loss: 0.6375\n",
      "Epoch [4950/10000], Loss: 0.6375\n",
      "Epoch [4960/10000], Loss: 0.6374\n",
      "Epoch [4970/10000], Loss: 0.6374\n",
      "Epoch [4980/10000], Loss: 0.6374\n",
      "Epoch [4990/10000], Loss: 0.6374\n",
      "Epoch [5000/10000], Loss: 0.6374\n",
      "Epoch [5010/10000], Loss: 0.6374\n",
      "Epoch [5020/10000], Loss: 0.6374\n",
      "Epoch [5030/10000], Loss: 0.6374\n",
      "Epoch [5040/10000], Loss: 0.6378\n",
      "Epoch [5050/10000], Loss: 0.6378\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5060/10000], Loss: 0.6374\n",
      "Epoch [5070/10000], Loss: 0.6373\n",
      "Epoch [5080/10000], Loss: 0.6373\n",
      "Epoch [5090/10000], Loss: 0.6373\n",
      "Epoch [5100/10000], Loss: 0.6373\n",
      "Epoch [5110/10000], Loss: 0.6373\n",
      "Epoch [5120/10000], Loss: 0.6373\n",
      "Epoch [5130/10000], Loss: 0.6373\n",
      "Epoch [5140/10000], Loss: 0.6373\n",
      "Epoch [5150/10000], Loss: 0.6373\n",
      "Epoch [5160/10000], Loss: 0.6373\n",
      "Epoch [5170/10000], Loss: 0.6373\n",
      "Epoch [5180/10000], Loss: 0.6372\n",
      "Epoch [5190/10000], Loss: 0.6372\n",
      "Epoch [5200/10000], Loss: 0.6372\n",
      "Epoch [5210/10000], Loss: 0.6372\n",
      "Epoch [5220/10000], Loss: 0.6372\n",
      "Epoch [5230/10000], Loss: 0.6372\n",
      "Epoch [5240/10000], Loss: 0.6372\n",
      "Epoch [5250/10000], Loss: 0.6372\n",
      "Epoch [5260/10000], Loss: 0.6372\n",
      "Epoch [5270/10000], Loss: 0.6372\n",
      "Epoch [5280/10000], Loss: 0.6371\n",
      "Epoch [5290/10000], Loss: 0.6371\n",
      "Epoch [5300/10000], Loss: 0.6371\n",
      "Epoch [5310/10000], Loss: 0.6372\n",
      "Epoch [5320/10000], Loss: 0.6371\n",
      "Epoch [5330/10000], Loss: 0.6372\n",
      "Epoch [5340/10000], Loss: 0.6371\n",
      "Epoch [5350/10000], Loss: 0.6371\n",
      "Epoch [5360/10000], Loss: 0.6371\n",
      "Epoch [5370/10000], Loss: 0.6371\n",
      "Epoch [5380/10000], Loss: 0.6371\n",
      "Epoch [5390/10000], Loss: 0.6370\n",
      "Epoch [5400/10000], Loss: 0.6370\n",
      "Epoch [5410/10000], Loss: 0.6370\n",
      "Epoch [5420/10000], Loss: 0.6370\n",
      "Epoch [5430/10000], Loss: 0.6370\n",
      "Epoch [5440/10000], Loss: 0.6370\n",
      "Epoch [5450/10000], Loss: 0.6370\n",
      "Epoch [5460/10000], Loss: 0.6370\n",
      "Epoch [5470/10000], Loss: 0.6370\n",
      "Epoch [5480/10000], Loss: 0.6370\n",
      "Epoch [5490/10000], Loss: 0.6370\n",
      "Epoch [5500/10000], Loss: 0.6369\n",
      "Epoch [5510/10000], Loss: 0.6369\n",
      "Epoch [5520/10000], Loss: 0.6369\n",
      "Epoch [5530/10000], Loss: 0.6369\n",
      "Epoch [5540/10000], Loss: 0.6369\n",
      "Epoch [5550/10000], Loss: 0.6369\n",
      "Epoch [5560/10000], Loss: 0.6369\n",
      "Epoch [5570/10000], Loss: 0.6369\n",
      "Epoch [5580/10000], Loss: 0.6369\n",
      "Epoch [5590/10000], Loss: 0.6369\n",
      "Epoch [5600/10000], Loss: 0.6372\n",
      "Epoch [5610/10000], Loss: 0.6370\n",
      "Epoch [5620/10000], Loss: 0.6368\n",
      "Epoch [5630/10000], Loss: 0.6368\n",
      "Epoch [5640/10000], Loss: 0.6368\n",
      "Epoch [5650/10000], Loss: 0.6368\n",
      "Epoch [5660/10000], Loss: 0.6368\n",
      "Epoch [5670/10000], Loss: 0.6368\n",
      "Epoch [5680/10000], Loss: 0.6368\n",
      "Epoch [5690/10000], Loss: 0.6368\n",
      "Epoch [5700/10000], Loss: 0.6368\n",
      "Epoch [5710/10000], Loss: 0.6367\n",
      "Epoch [5720/10000], Loss: 0.6367\n",
      "Epoch [5730/10000], Loss: 0.6367\n",
      "Epoch [5740/10000], Loss: 0.6367\n",
      "Epoch [5750/10000], Loss: 0.6367\n",
      "Epoch [5760/10000], Loss: 0.6367\n",
      "Epoch [5770/10000], Loss: 0.6367\n",
      "Epoch [5780/10000], Loss: 0.6367\n",
      "Epoch [5790/10000], Loss: 0.6367\n",
      "Epoch [5800/10000], Loss: 0.6367\n",
      "Epoch [5810/10000], Loss: 0.6367\n",
      "Epoch [5820/10000], Loss: 0.6369\n",
      "Epoch [5830/10000], Loss: 0.6367\n",
      "Epoch [5840/10000], Loss: 0.6367\n",
      "Epoch [5850/10000], Loss: 0.6367\n",
      "Epoch [5860/10000], Loss: 0.6366\n",
      "Epoch [5870/10000], Loss: 0.6366\n",
      "Epoch [5880/10000], Loss: 0.6366\n",
      "Epoch [5890/10000], Loss: 0.6366\n",
      "Epoch [5900/10000], Loss: 0.6366\n",
      "Epoch [5910/10000], Loss: 0.6366\n",
      "Epoch [5920/10000], Loss: 0.6365\n",
      "Epoch [5930/10000], Loss: 0.6365\n",
      "Epoch [5940/10000], Loss: 0.6365\n",
      "Epoch [5950/10000], Loss: 0.6365\n",
      "Epoch [5960/10000], Loss: 0.6365\n",
      "Epoch [5970/10000], Loss: 0.6365\n",
      "Epoch [5980/10000], Loss: 0.6365\n",
      "Epoch [5990/10000], Loss: 0.6365\n",
      "Epoch [6000/10000], Loss: 0.6365\n",
      "Epoch [6010/10000], Loss: 0.6365\n",
      "Epoch [6020/10000], Loss: 0.6365\n",
      "Epoch [6030/10000], Loss: 0.6364\n",
      "Epoch [6040/10000], Loss: 0.6364\n",
      "Epoch [6050/10000], Loss: 0.6364\n",
      "Epoch [6060/10000], Loss: 0.6366\n",
      "Epoch [6070/10000], Loss: 0.6365\n",
      "Epoch [6080/10000], Loss: 0.6364\n",
      "Epoch [6090/10000], Loss: 0.6364\n",
      "Epoch [6100/10000], Loss: 0.6364\n",
      "Epoch [6110/10000], Loss: 0.6364\n",
      "Epoch [6120/10000], Loss: 0.6364\n",
      "Epoch [6130/10000], Loss: 0.6363\n",
      "Epoch [6140/10000], Loss: 0.6363\n",
      "Epoch [6150/10000], Loss: 0.6363\n",
      "Epoch [6160/10000], Loss: 0.6363\n",
      "Epoch [6170/10000], Loss: 0.6363\n",
      "Epoch [6180/10000], Loss: 0.6363\n",
      "Epoch [6190/10000], Loss: 0.6363\n",
      "Epoch [6200/10000], Loss: 0.6363\n",
      "Epoch [6210/10000], Loss: 0.6363\n",
      "Epoch [6220/10000], Loss: 0.6363\n",
      "Epoch [6230/10000], Loss: 0.6362\n",
      "Epoch [6240/10000], Loss: 0.6362\n",
      "Epoch [6250/10000], Loss: 0.6362\n",
      "Epoch [6260/10000], Loss: 0.6362\n",
      "Epoch [6270/10000], Loss: 0.6362\n",
      "Epoch [6280/10000], Loss: 0.6362\n",
      "Epoch [6290/10000], Loss: 0.6362\n",
      "Epoch [6300/10000], Loss: 0.6362\n",
      "Epoch [6310/10000], Loss: 0.6363\n",
      "Epoch [6320/10000], Loss: 0.6362\n",
      "Epoch [6330/10000], Loss: 0.6363\n",
      "Epoch [6340/10000], Loss: 0.6362\n",
      "Epoch [6350/10000], Loss: 0.6361\n",
      "Epoch [6360/10000], Loss: 0.6361\n",
      "Epoch [6370/10000], Loss: 0.6361\n",
      "Epoch [6380/10000], Loss: 0.6361\n",
      "Epoch [6390/10000], Loss: 0.6361\n",
      "Epoch [6400/10000], Loss: 0.6361\n",
      "Epoch [6410/10000], Loss: 0.6361\n",
      "Epoch [6420/10000], Loss: 0.6361\n",
      "Epoch [6430/10000], Loss: 0.6361\n",
      "Epoch [6440/10000], Loss: 0.6360\n",
      "Epoch [6450/10000], Loss: 0.6360\n",
      "Epoch [6460/10000], Loss: 0.6360\n",
      "Epoch [6470/10000], Loss: 0.6360\n",
      "Epoch [6480/10000], Loss: 0.6360\n",
      "Epoch [6490/10000], Loss: 0.6360\n",
      "Epoch [6500/10000], Loss: 0.6360\n",
      "Epoch [6510/10000], Loss: 0.6360\n",
      "Epoch [6520/10000], Loss: 0.6360\n",
      "Epoch [6530/10000], Loss: 0.6360\n",
      "Epoch [6540/10000], Loss: 0.6360\n",
      "Epoch [6550/10000], Loss: 0.6367\n",
      "Epoch [6560/10000], Loss: 0.6362\n",
      "Epoch [6570/10000], Loss: 0.6360\n",
      "Epoch [6580/10000], Loss: 0.6359\n",
      "Epoch [6590/10000], Loss: 0.6359\n",
      "Epoch [6600/10000], Loss: 0.6359\n",
      "Epoch [6610/10000], Loss: 0.6359\n",
      "Epoch [6620/10000], Loss: 0.6359\n",
      "Epoch [6630/10000], Loss: 0.6359\n",
      "Epoch [6640/10000], Loss: 0.6358\n",
      "Epoch [6650/10000], Loss: 0.6358\n",
      "Epoch [6660/10000], Loss: 0.6358\n",
      "Epoch [6670/10000], Loss: 0.6358\n",
      "Epoch [6680/10000], Loss: 0.6358\n",
      "Epoch [6690/10000], Loss: 0.6358\n",
      "Epoch [6700/10000], Loss: 0.6358\n",
      "Epoch [6710/10000], Loss: 0.6358\n",
      "Epoch [6720/10000], Loss: 0.6358\n",
      "Epoch [6730/10000], Loss: 0.6358\n",
      "Epoch [6740/10000], Loss: 0.6357\n",
      "Epoch [6750/10000], Loss: 0.6357\n",
      "Epoch [6760/10000], Loss: 0.6357\n",
      "Epoch [6770/10000], Loss: 0.6357\n",
      "Epoch [6780/10000], Loss: 0.6358\n",
      "Epoch [6790/10000], Loss: 0.6359\n",
      "Epoch [6800/10000], Loss: 0.6359\n",
      "Epoch [6810/10000], Loss: 0.6357\n",
      "Epoch [6820/10000], Loss: 0.6357\n",
      "Epoch [6830/10000], Loss: 0.6357\n",
      "Epoch [6840/10000], Loss: 0.6356\n",
      "Epoch [6850/10000], Loss: 0.6356\n",
      "Epoch [6860/10000], Loss: 0.6356\n",
      "Epoch [6870/10000], Loss: 0.6356\n",
      "Epoch [6880/10000], Loss: 0.6356\n",
      "Epoch [6890/10000], Loss: 0.6356\n",
      "Epoch [6900/10000], Loss: 0.6356\n",
      "Epoch [6910/10000], Loss: 0.6356\n",
      "Epoch [6920/10000], Loss: 0.6356\n",
      "Epoch [6930/10000], Loss: 0.6356\n",
      "Epoch [6940/10000], Loss: 0.6355\n",
      "Epoch [6950/10000], Loss: 0.6355\n",
      "Epoch [6960/10000], Loss: 0.6355\n",
      "Epoch [6970/10000], Loss: 0.6355\n",
      "Epoch [6980/10000], Loss: 0.6355\n",
      "Epoch [6990/10000], Loss: 0.6355\n",
      "Epoch [7000/10000], Loss: 0.6355\n",
      "Epoch [7010/10000], Loss: 0.6355\n",
      "Epoch [7020/10000], Loss: 0.6364\n",
      "Epoch [7030/10000], Loss: 0.6356\n",
      "Epoch [7040/10000], Loss: 0.6356\n",
      "Epoch [7050/10000], Loss: 0.6354\n",
      "Epoch [7060/10000], Loss: 0.6354\n",
      "Epoch [7070/10000], Loss: 0.6354\n",
      "Epoch [7080/10000], Loss: 0.6354\n",
      "Epoch [7090/10000], Loss: 0.6354\n",
      "Epoch [7100/10000], Loss: 0.6354\n",
      "Epoch [7110/10000], Loss: 0.6354\n",
      "Epoch [7120/10000], Loss: 0.6354\n",
      "Epoch [7130/10000], Loss: 0.6353\n",
      "Epoch [7140/10000], Loss: 0.6353\n",
      "Epoch [7150/10000], Loss: 0.6353\n",
      "Epoch [7160/10000], Loss: 0.6353\n",
      "Epoch [7170/10000], Loss: 0.6353\n",
      "Epoch [7180/10000], Loss: 0.6353\n",
      "Epoch [7190/10000], Loss: 0.6353\n",
      "Epoch [7200/10000], Loss: 0.6353\n",
      "Epoch [7210/10000], Loss: 0.6353\n",
      "Epoch [7220/10000], Loss: 0.6353\n",
      "Epoch [7230/10000], Loss: 0.6352\n",
      "Epoch [7240/10000], Loss: 0.6352\n",
      "Epoch [7250/10000], Loss: 0.6359\n",
      "Epoch [7260/10000], Loss: 0.6355\n",
      "Epoch [7270/10000], Loss: 0.6352\n",
      "Epoch [7280/10000], Loss: 0.6352\n",
      "Epoch [7290/10000], Loss: 0.6352\n",
      "Epoch [7300/10000], Loss: 0.6352\n",
      "Epoch [7310/10000], Loss: 0.6352\n",
      "Epoch [7320/10000], Loss: 0.6352\n",
      "Epoch [7330/10000], Loss: 0.6351\n",
      "Epoch [7340/10000], Loss: 0.6351\n",
      "Epoch [7350/10000], Loss: 0.6351\n",
      "Epoch [7360/10000], Loss: 0.6351\n",
      "Epoch [7370/10000], Loss: 0.6351\n",
      "Epoch [7380/10000], Loss: 0.6351\n",
      "Epoch [7390/10000], Loss: 0.6351\n",
      "Epoch [7400/10000], Loss: 0.6351\n",
      "Epoch [7410/10000], Loss: 0.6351\n",
      "Epoch [7420/10000], Loss: 0.6350\n",
      "Epoch [7430/10000], Loss: 0.6350\n",
      "Epoch [7440/10000], Loss: 0.6350\n",
      "Epoch [7450/10000], Loss: 0.6350\n",
      "Epoch [7460/10000], Loss: 0.6351\n",
      "Epoch [7470/10000], Loss: 0.6356\n",
      "Epoch [7480/10000], Loss: 0.6350\n",
      "Epoch [7490/10000], Loss: 0.6350\n",
      "Epoch [7500/10000], Loss: 0.6350\n",
      "Epoch [7510/10000], Loss: 0.6350\n",
      "Epoch [7520/10000], Loss: 0.6349\n",
      "Epoch [7530/10000], Loss: 0.6349\n",
      "Epoch [7540/10000], Loss: 0.6349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7550/10000], Loss: 0.6349\n",
      "Epoch [7560/10000], Loss: 0.6349\n",
      "Epoch [7570/10000], Loss: 0.6349\n",
      "Epoch [7580/10000], Loss: 0.6349\n",
      "Epoch [7590/10000], Loss: 0.6349\n",
      "Epoch [7600/10000], Loss: 0.6349\n",
      "Epoch [7610/10000], Loss: 0.6349\n",
      "Epoch [7620/10000], Loss: 0.6348\n",
      "Epoch [7630/10000], Loss: 0.6348\n",
      "Epoch [7640/10000], Loss: 0.6348\n",
      "Epoch [7650/10000], Loss: 0.6348\n",
      "Epoch [7660/10000], Loss: 0.6348\n",
      "Epoch [7670/10000], Loss: 0.6348\n",
      "Epoch [7680/10000], Loss: 0.6348\n",
      "Epoch [7690/10000], Loss: 0.6348\n",
      "Epoch [7700/10000], Loss: 0.6357\n",
      "Epoch [7710/10000], Loss: 0.6349\n",
      "Epoch [7720/10000], Loss: 0.6347\n",
      "Epoch [7730/10000], Loss: 0.6347\n",
      "Epoch [7740/10000], Loss: 0.6347\n",
      "Epoch [7750/10000], Loss: 0.6347\n",
      "Epoch [7760/10000], Loss: 0.6347\n",
      "Epoch [7770/10000], Loss: 0.6347\n",
      "Epoch [7780/10000], Loss: 0.6347\n",
      "Epoch [7790/10000], Loss: 0.6347\n",
      "Epoch [7800/10000], Loss: 0.6347\n",
      "Epoch [7810/10000], Loss: 0.6346\n",
      "Epoch [7820/10000], Loss: 0.6346\n",
      "Epoch [7830/10000], Loss: 0.6346\n",
      "Epoch [7840/10000], Loss: 0.6346\n",
      "Epoch [7850/10000], Loss: 0.6346\n",
      "Epoch [7860/10000], Loss: 0.6346\n",
      "Epoch [7870/10000], Loss: 0.6346\n",
      "Epoch [7880/10000], Loss: 0.6346\n",
      "Epoch [7890/10000], Loss: 0.6346\n",
      "Epoch [7900/10000], Loss: 0.6345\n",
      "Epoch [7910/10000], Loss: 0.6345\n",
      "Epoch [7920/10000], Loss: 0.6345\n",
      "Epoch [7930/10000], Loss: 0.6345\n",
      "Epoch [7940/10000], Loss: 0.6345\n",
      "Epoch [7950/10000], Loss: 0.6345\n",
      "Epoch [7960/10000], Loss: 0.6352\n",
      "Epoch [7970/10000], Loss: 0.6349\n",
      "Epoch [7980/10000], Loss: 0.6346\n",
      "Epoch [7990/10000], Loss: 0.6345\n",
      "Epoch [8000/10000], Loss: 0.6344\n",
      "Epoch [8010/10000], Loss: 0.6344\n",
      "Epoch [8020/10000], Loss: 0.6344\n",
      "Epoch [8030/10000], Loss: 0.6344\n",
      "Epoch [8040/10000], Loss: 0.6344\n",
      "Epoch [8050/10000], Loss: 0.6344\n",
      "Epoch [8060/10000], Loss: 0.6344\n",
      "Epoch [8070/10000], Loss: 0.6344\n",
      "Epoch [8080/10000], Loss: 0.6344\n",
      "Epoch [8090/10000], Loss: 0.6343\n",
      "Epoch [8100/10000], Loss: 0.6343\n",
      "Epoch [8110/10000], Loss: 0.6343\n",
      "Epoch [8120/10000], Loss: 0.6343\n",
      "Epoch [8130/10000], Loss: 0.6343\n",
      "Epoch [8140/10000], Loss: 0.6343\n",
      "Epoch [8150/10000], Loss: 0.6343\n",
      "Epoch [8160/10000], Loss: 0.6343\n",
      "Epoch [8170/10000], Loss: 0.6343\n",
      "Epoch [8180/10000], Loss: 0.6342\n",
      "Epoch [8190/10000], Loss: 0.6342\n",
      "Epoch [8200/10000], Loss: 0.6342\n",
      "Epoch [8210/10000], Loss: 0.6342\n",
      "Epoch [8220/10000], Loss: 0.6344\n",
      "Epoch [8230/10000], Loss: 0.6342\n",
      "Epoch [8240/10000], Loss: 0.6342\n",
      "Epoch [8250/10000], Loss: 0.6342\n",
      "Epoch [8260/10000], Loss: 0.6342\n",
      "Epoch [8270/10000], Loss: 0.6342\n",
      "Epoch [8280/10000], Loss: 0.6341\n",
      "Epoch [8290/10000], Loss: 0.6341\n",
      "Epoch [8300/10000], Loss: 0.6341\n",
      "Epoch [8310/10000], Loss: 0.6341\n",
      "Epoch [8320/10000], Loss: 0.6341\n",
      "Epoch [8330/10000], Loss: 0.6341\n",
      "Epoch [8340/10000], Loss: 0.6341\n",
      "Epoch [8350/10000], Loss: 0.6341\n",
      "Epoch [8360/10000], Loss: 0.6340\n",
      "Epoch [8370/10000], Loss: 0.6340\n",
      "Epoch [8380/10000], Loss: 0.6340\n",
      "Epoch [8390/10000], Loss: 0.6340\n",
      "Epoch [8400/10000], Loss: 0.6340\n",
      "Epoch [8410/10000], Loss: 0.6340\n",
      "Epoch [8420/10000], Loss: 0.6340\n",
      "Epoch [8430/10000], Loss: 0.6340\n",
      "Epoch [8440/10000], Loss: 0.6340\n",
      "Epoch [8450/10000], Loss: 0.6339\n",
      "Epoch [8460/10000], Loss: 0.6339\n",
      "Epoch [8470/10000], Loss: 0.6339\n",
      "Epoch [8480/10000], Loss: 0.6339\n",
      "Epoch [8490/10000], Loss: 0.6346\n",
      "Epoch [8500/10000], Loss: 0.6343\n",
      "Epoch [8510/10000], Loss: 0.6340\n",
      "Epoch [8520/10000], Loss: 0.6339\n",
      "Epoch [8530/10000], Loss: 0.6339\n",
      "Epoch [8540/10000], Loss: 0.6339\n",
      "Epoch [8550/10000], Loss: 0.6338\n",
      "Epoch [8560/10000], Loss: 0.6338\n",
      "Epoch [8570/10000], Loss: 0.6338\n",
      "Epoch [8580/10000], Loss: 0.6338\n",
      "Epoch [8590/10000], Loss: 0.6338\n",
      "Epoch [8600/10000], Loss: 0.6338\n",
      "Epoch [8610/10000], Loss: 0.6338\n",
      "Epoch [8620/10000], Loss: 0.6338\n",
      "Epoch [8630/10000], Loss: 0.6338\n",
      "Epoch [8640/10000], Loss: 0.6337\n",
      "Epoch [8650/10000], Loss: 0.6337\n",
      "Epoch [8660/10000], Loss: 0.6337\n",
      "Epoch [8670/10000], Loss: 0.6337\n",
      "Epoch [8680/10000], Loss: 0.6337\n",
      "Epoch [8690/10000], Loss: 0.6337\n",
      "Epoch [8700/10000], Loss: 0.6337\n",
      "Epoch [8710/10000], Loss: 0.6337\n",
      "Epoch [8720/10000], Loss: 0.6337\n",
      "Epoch [8730/10000], Loss: 0.6336\n",
      "Epoch [8740/10000], Loss: 0.6336\n",
      "Epoch [8750/10000], Loss: 0.6336\n",
      "Epoch [8760/10000], Loss: 0.6336\n",
      "Epoch [8770/10000], Loss: 0.6336\n",
      "Epoch [8780/10000], Loss: 0.6336\n",
      "Epoch [8790/10000], Loss: 0.6337\n",
      "Epoch [8800/10000], Loss: 0.6336\n",
      "Epoch [8810/10000], Loss: 0.6337\n",
      "Epoch [8820/10000], Loss: 0.6336\n",
      "Epoch [8830/10000], Loss: 0.6336\n",
      "Epoch [8840/10000], Loss: 0.6335\n",
      "Epoch [8850/10000], Loss: 0.6335\n",
      "Epoch [8860/10000], Loss: 0.6335\n",
      "Epoch [8870/10000], Loss: 0.6335\n",
      "Epoch [8880/10000], Loss: 0.6335\n",
      "Epoch [8890/10000], Loss: 0.6335\n",
      "Epoch [8900/10000], Loss: 0.6335\n",
      "Epoch [8910/10000], Loss: 0.6334\n",
      "Epoch [8920/10000], Loss: 0.6334\n",
      "Epoch [8930/10000], Loss: 0.6334\n",
      "Epoch [8940/10000], Loss: 0.6334\n",
      "Epoch [8950/10000], Loss: 0.6334\n",
      "Epoch [8960/10000], Loss: 0.6334\n",
      "Epoch [8970/10000], Loss: 0.6334\n",
      "Epoch [8980/10000], Loss: 0.6334\n",
      "Epoch [8990/10000], Loss: 0.6334\n",
      "Epoch [9000/10000], Loss: 0.6333\n",
      "Epoch [9010/10000], Loss: 0.6333\n",
      "Epoch [9020/10000], Loss: 0.6333\n",
      "Epoch [9030/10000], Loss: 0.6333\n",
      "Epoch [9040/10000], Loss: 0.6333\n",
      "Epoch [9050/10000], Loss: 0.6333\n",
      "Epoch [9060/10000], Loss: 0.6333\n",
      "Epoch [9070/10000], Loss: 0.6333\n",
      "Epoch [9080/10000], Loss: 0.6341\n",
      "Epoch [9090/10000], Loss: 0.6335\n",
      "Epoch [9100/10000], Loss: 0.6333\n",
      "Epoch [9110/10000], Loss: 0.6332\n",
      "Epoch [9120/10000], Loss: 0.6332\n",
      "Epoch [9130/10000], Loss: 0.6332\n",
      "Epoch [9140/10000], Loss: 0.6332\n",
      "Epoch [9150/10000], Loss: 0.6332\n",
      "Epoch [9160/10000], Loss: 0.6332\n",
      "Epoch [9170/10000], Loss: 0.6331\n",
      "Epoch [9180/10000], Loss: 0.6331\n",
      "Epoch [9190/10000], Loss: 0.6331\n",
      "Epoch [9200/10000], Loss: 0.6331\n",
      "Epoch [9210/10000], Loss: 0.6331\n",
      "Epoch [9220/10000], Loss: 0.6331\n",
      "Epoch [9230/10000], Loss: 0.6331\n",
      "Epoch [9240/10000], Loss: 0.6331\n",
      "Epoch [9250/10000], Loss: 0.6331\n",
      "Epoch [9260/10000], Loss: 0.6330\n",
      "Epoch [9270/10000], Loss: 0.6330\n",
      "Epoch [9280/10000], Loss: 0.6330\n",
      "Epoch [9290/10000], Loss: 0.6330\n",
      "Epoch [9300/10000], Loss: 0.6330\n",
      "Epoch [9310/10000], Loss: 0.6330\n",
      "Epoch [9320/10000], Loss: 0.6330\n",
      "Epoch [9330/10000], Loss: 0.6330\n",
      "Epoch [9340/10000], Loss: 0.6330\n",
      "Epoch [9350/10000], Loss: 0.6333\n",
      "Epoch [9360/10000], Loss: 0.6333\n",
      "Epoch [9370/10000], Loss: 0.6329\n",
      "Epoch [9380/10000], Loss: 0.6329\n",
      "Epoch [9390/10000], Loss: 0.6329\n",
      "Epoch [9400/10000], Loss: 0.6329\n",
      "Epoch [9410/10000], Loss: 0.6329\n",
      "Epoch [9420/10000], Loss: 0.6329\n",
      "Epoch [9430/10000], Loss: 0.6329\n",
      "Epoch [9440/10000], Loss: 0.6328\n",
      "Epoch [9450/10000], Loss: 0.6328\n",
      "Epoch [9460/10000], Loss: 0.6328\n",
      "Epoch [9470/10000], Loss: 0.6328\n",
      "Epoch [9480/10000], Loss: 0.6328\n",
      "Epoch [9490/10000], Loss: 0.6328\n",
      "Epoch [9500/10000], Loss: 0.6328\n",
      "Epoch [9510/10000], Loss: 0.6328\n",
      "Epoch [9520/10000], Loss: 0.6327\n",
      "Epoch [9530/10000], Loss: 0.6327\n",
      "Epoch [9540/10000], Loss: 0.6327\n",
      "Epoch [9550/10000], Loss: 0.6327\n",
      "Epoch [9560/10000], Loss: 0.6327\n",
      "Epoch [9570/10000], Loss: 0.6327\n",
      "Epoch [9580/10000], Loss: 0.6327\n",
      "Epoch [9590/10000], Loss: 0.6327\n",
      "Epoch [9600/10000], Loss: 0.6327\n",
      "Epoch [9610/10000], Loss: 0.6338\n",
      "Epoch [9620/10000], Loss: 0.6326\n",
      "Epoch [9630/10000], Loss: 0.6327\n",
      "Epoch [9640/10000], Loss: 0.6326\n",
      "Epoch [9650/10000], Loss: 0.6326\n",
      "Epoch [9660/10000], Loss: 0.6326\n",
      "Epoch [9670/10000], Loss: 0.6326\n",
      "Epoch [9680/10000], Loss: 0.6326\n",
      "Epoch [9690/10000], Loss: 0.6326\n",
      "Epoch [9700/10000], Loss: 0.6325\n",
      "Epoch [9710/10000], Loss: 0.6325\n",
      "Epoch [9720/10000], Loss: 0.6325\n",
      "Epoch [9730/10000], Loss: 0.6325\n",
      "Epoch [9740/10000], Loss: 0.6325\n",
      "Epoch [9750/10000], Loss: 0.6325\n",
      "Epoch [9760/10000], Loss: 0.6325\n",
      "Epoch [9770/10000], Loss: 0.6325\n",
      "Epoch [9780/10000], Loss: 0.6324\n",
      "Epoch [9790/10000], Loss: 0.6324\n",
      "Epoch [9800/10000], Loss: 0.6324\n",
      "Epoch [9810/10000], Loss: 0.6324\n",
      "Epoch [9820/10000], Loss: 0.6324\n",
      "Epoch [9830/10000], Loss: 0.6324\n",
      "Epoch [9840/10000], Loss: 0.6324\n",
      "Epoch [9850/10000], Loss: 0.6324\n",
      "Epoch [9860/10000], Loss: 0.6324\n",
      "Epoch [9870/10000], Loss: 0.6323\n",
      "Epoch [9880/10000], Loss: 0.6327\n",
      "Epoch [9890/10000], Loss: 0.6328\n",
      "Epoch [9900/10000], Loss: 0.6324\n",
      "Epoch [9910/10000], Loss: 0.6323\n",
      "Epoch [9920/10000], Loss: 0.6323\n",
      "Epoch [9930/10000], Loss: 0.6323\n",
      "Epoch [9940/10000], Loss: 0.6323\n",
      "Epoch [9950/10000], Loss: 0.6322\n",
      "Epoch [9960/10000], Loss: 0.6322\n",
      "Epoch [9970/10000], Loss: 0.6322\n",
      "Epoch [9980/10000], Loss: 0.6322\n",
      "Epoch [9990/10000], Loss: 0.6322\n",
      "Epoch [10000/10000], Loss: 0.6322\n",
      "Predicted Labels:\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "        0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 10000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = rnn(data)\n",
    "    loss = loss_fn(outputs.squeeze(dim=0), labels.squeeze(dim=0))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Testing\n",
    "test_outputs = rnn(data)\n",
    "_, predicted = torch.max(test_outputs.squeeze(dim=0).data, 1)\n",
    "\n",
    "# Print the predicted labels\n",
    "print(\"Predicted Labels:\")\n",
    "print(predicted)\n",
    "\n",
    "losses_list.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "58cc0cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5d0e9a8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'loss')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(epochs_list,losses_list,color='r')\n",
    "plt.title('epochs vs loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e78d5b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "torch.save(rnn.state_dict(), 'trained_model_rnn.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9b3e09c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Load the saved model\n",
    "# loaded_model = RNN(num_channels, hidden_size, num_classes)\n",
    "# loaded_model.load_state_dict(torch.load('trained_model_rnn.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f23a29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d594448c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 57.49999999999999%\n"
     ]
    }
   ],
   "source": [
    "# Read the CSV file\n",
    "data_frame = pd.read_csv('/home/saqib/projects/neuromorphicComputing/emg_data_class1_test.csv')\n",
    "\n",
    "# Convert the DataFrame to a tensor\n",
    "class1_data = torch.tensor(data_frame.values, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# Read the CSV file\n",
    "data_frame = pd.read_csv('/home/saqib/projects/neuromorphicComputing/emg_data_class2_test.csv')\n",
    "\n",
    "# Convert the DataFrame to a tensor\n",
    "class2_data = torch.tensor(data_frame.values, dtype=torch.float32)\n",
    "\n",
    "# Assign labels to each class\n",
    "class1_labels = torch.zeros(100, dtype=torch.long)\n",
    "class2_labels = torch.ones(100, dtype=torch.long)\n",
    "\n",
    "\n",
    "# Concatenate the data and labels\n",
    "data = torch.cat((class1_data, class2_data), dim=0)\n",
    "labels = torch.cat((class1_labels, class2_labels), dim=0)\n",
    "\n",
    "# Reshape the data for RNN compatibility\n",
    "data = data.unsqueeze(dim=0)  # Add a batch dimension\n",
    "data = data.permute(1, 0, 2)  # Reshape to (sequence_length, batch_size, num_channels)\n",
    "\n",
    "# Ensure labels have the same batch size as data\n",
    "labels = labels.unsqueeze(dim=0)\n",
    "\n",
    "# Calculate accuracy\n",
    "\n",
    "correct = 0\n",
    "total = labels.size(1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = rnn(data)\n",
    "    _, predicted = torch.max(outputs.squeeze(dim=0).data, 1)\n",
    "    correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = correct / total * 100\n",
    "print(f\"Accuracy: {accuracy}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "42b92ee4",
   "metadata": {},
   "source": [
    "<img src = '/home/saqib/Pictures/Screenshots/lossVSepoch.png' alt=\"loss vs epochs\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "891f9e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Labels:\n",
      "tensor([0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        0, 1, 1, 1, 1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "# Print the predicted labels\n",
    "print(\"Predicted Labels:\")\n",
    "print(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "373451e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.693205714225769, 0.6931171417236328, 0.6930366158485413, 0.6916710734367371, 0.6805242300033569, 0.6656092405319214, 0.65693598985672, 0.6505345106124878, 0.6489424705505371, 0.6485673189163208, 0.648534893989563, 0.6485234498977661, 0.6484548449516296, 0.6479994654655457, 0.6460892558097839, 0.6414579153060913, 0.6321865320205688]\n"
     ]
    }
   ],
   "source": [
    "print(losses_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0dfc3f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
